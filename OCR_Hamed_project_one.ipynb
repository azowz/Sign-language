{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azowz/Sign-language/blob/main/OCR_Hamed_project_one.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tarfile\n",
        "import shutil\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "\n",
        "def find_dataset_tar():\n",
        "    \"\"\"Search for IIIT5K dataset tar file in common locations\"\"\"\n",
        "    # First check current directory\n",
        "    for filename in os.listdir(\".\"):\n",
        "        if \"iiit5k\" in filename.lower() and (filename.endswith(\".tar\") or filename.endswith(\".tar.gz\")):\n",
        "            return filename\n",
        "\n",
        "    # Then check sample_data directory (common in Colab)\n",
        "    sample_data = \"sample_data\"\n",
        "    if os.path.exists(sample_data):\n",
        "        for filename in os.listdir(sample_data):\n",
        "            if \"iiit5k\" in filename.lower() and (filename.endswith(\".tar\") or filename.endswith(\".tar.gz\")):\n",
        "                return os.path.join(sample_data, filename)\n",
        "\n",
        "    # If still not found, check for any tar file\n",
        "    for filename in os.listdir(\".\"):\n",
        "        if filename.endswith(\".tar\") or filename.endswith(\".tar.gz\"):\n",
        "            return filename\n",
        "\n",
        "    # Final fallback: look for extracted dataset\n",
        "    if os.path.exists(\"IIIT5K\"):\n",
        "        return \"ALREADY_EXTRACTED\"\n",
        "\n",
        "    return None\n",
        "\n",
        "def extract_and_verify_dataset():\n",
        "    \"\"\"Extract the IIIT5K dataset with flexible location handling\"\"\"\n",
        "    # Try to find the dataset\n",
        "    tar_path = find_dataset_tar()\n",
        "\n",
        "    if not tar_path:\n",
        "        print(\"=\"*50)\n",
        "        print(\"ERROR: IIIT5K dataset not found!\")\n",
        "        print(\"Please upload the dataset using:\")\n",
        "        print(\"1. Google Drive: Mount Drive and specify path\")\n",
        "        print(\"2. Direct upload: Click 'Upload' in Colab file browser\")\n",
        "        print(\"3. Or download with: !wget http://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-5k-word-dataset/IIIT5K-Word_V3.0.tar\")\n",
        "        print(\"=\"*50)\n",
        "        return None\n",
        "\n",
        "    # Create output directory\n",
        "    output_dir = \"iiit5k_dataset\"\n",
        "    if os.path.exists(output_dir):\n",
        "        shutil.rmtree(output_dir)\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "    # Handle already extracted dataset\n",
        "    if tar_path == \"ALREADY_EXTRACTED\":\n",
        "        print(\"‚úì Found pre-extracted dataset in 'IIIT5K' folder\")\n",
        "        print(\"Copying to working directory...\")\n",
        "        for item in os.listdir(\"IIIT5K\"):\n",
        "            shutil.move(os.path.join(\"IIIT5K\", item), output_dir)\n",
        "        shutil.rmtree(\"IIIT5K\")\n",
        "        return verify_dataset_structure(output_dir)\n",
        "\n",
        "    # Extract the tar file\n",
        "    print(f\"‚úì Found dataset: {tar_path}\")\n",
        "    print(f\"Extracting to {output_dir}...\")\n",
        "    try:\n",
        "        if tar_path.endswith(\".tar.gz\"):\n",
        "            with tarfile.open(tar_path, \"r:gz\") as tar:\n",
        "                tar.extractall(output_dir)\n",
        "        else:\n",
        "            with tarfile.open(tar_path, \"r\") as tar:\n",
        "                tar.extractall(output_dir)\n",
        "        print(\"‚úì Extraction completed successfully!\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚úó Error extracting file: {e}\")\n",
        "        return None\n",
        "\n",
        "    return verify_dataset_structure(output_dir)\n",
        "\n",
        "def verify_dataset_structure(output_dir):\n",
        "    \"\"\"Verify the dataset structure after extraction\"\"\"\n",
        "    # Standard IIIT5K structure\n",
        "    train_dir = None\n",
        "    test_dir = None\n",
        "    gt_file = None\n",
        "\n",
        "    # Search for components\n",
        "    for root, dirs, files in os.walk(output_dir):\n",
        "        for dir_name in dirs:\n",
        "            if \"train\" in dir_name.lower() and not any(x in dir_name.lower() for x in [\"test\", \"val\"]):\n",
        "                train_dir = os.path.join(root, dir_name)\n",
        "            if \"test\" in dir_name.lower() and not any(x in dir_name.lower() for x in [\"train\", \"val\"]):\n",
        "                test_dir = os.path.join(root, dir_name)\n",
        "\n",
        "        for file in files:\n",
        "            if \"gt\" in file.lower() and file.endswith((\".mat\", \".txt\")):\n",
        "                gt_file = os.path.join(root, file)\n",
        "\n",
        "    # Check image counts\n",
        "    train_count = 0\n",
        "    test_count = 0\n",
        "    if train_dir and os.path.exists(train_dir):\n",
        "        train_count = len([f for f in os.listdir(train_dir)\n",
        "                          if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "    if test_dir and os.path.exists(test_dir):\n",
        "        test_count = len([f for f in os.listdir(test_dir)\n",
        "                         if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
        "\n",
        "    # Create verification report\n",
        "    report = {\n",
        "        \"base_dir\": output_dir,\n",
        "        \"train_dir\": train_dir,\n",
        "        \"test_dir\": test_dir,\n",
        "        \"gt_file\": gt_file,\n",
        "        \"train_count\": train_count,\n",
        "        \"test_count\": test_count,\n",
        "        \"valid\": bool(train_dir and test_dir and gt_file and (train_count > 0 or test_count > 0))\n",
        "    }\n",
        "\n",
        "    # Print verification\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DATASET VERIFICATION\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"Training images: {train_count} {'‚úì' if train_count > 0 else '‚úó'}\")\n",
        "    print(f\"Test images: {test_count} {'‚úì' if test_count > 0 else '‚úó'}\")\n",
        "    print(f\"Ground truth: {os.path.basename(gt_file) if gt_file else 'Not found'} {'‚úì' if gt_file else '‚úó'}\")\n",
        "\n",
        "    if report[\"valid\"]:\n",
        "        print(\"\\n‚úì Dataset verified successfully!\")\n",
        "        return report\n",
        "    else:\n",
        "        print(\"\\n‚úó Dataset verification failed. Common issues:\")\n",
        "        print(\"  - Incorrect dataset format (should be IIIT5K-Word_V3.0)\")\n",
        "        print(\"  - Missing ground truth files\")\n",
        "        print(\"  - Corrupted extraction\")\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"=\"*50)\n",
        "    print(\"IIIT5K DATASET LOADER - SMART FIX\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    # Attempt to load dataset\n",
        "    report = extract_and_verify_dataset()\n",
        "\n",
        "    if not report:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"NEXT STEPS:\")\n",
        "        print(\"1. Download dataset from: http://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-5k-word-dataset/\")\n",
        "        print(\"2. Upload IIIT5K-Word_V3.0.tar to Colab\")\n",
        "        print(\"   - Click folder icon > 'Upload' in left sidebar\")\n",
        "        print(\"3. OR use direct download command:\")\n",
        "        print(\"   !wget http://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-5k-word-dataset/IIIT5K-Word_V3.0.tar\")\n",
        "        print(\"=\"*50)\n",
        "        exit(1)\n",
        "\n",
        "    # Generate dataset loader code\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"DATASET LOADER READY - USE THIS IN YOUR OCR CODE\")\n",
        "    print(\"=\"*50)\n",
        "    print(f\"# Base directory: {os.path.abspath(report['base_dir'])}\")\n",
        "    print(\"from torch.utils.data import Dataset\\n\")\n",
        "    print(\"class IIIT5KDataset(Dataset):\")\n",
        "    print(\"    def __init__(self, root_dir, split='train'):\")\n",
        "    print(\"        self.root_dir = root_dir\")\n",
        "    print(\"        self.split = split\")\n",
        "    print(\"        \")\n",
        "    print(\"        # Load ground truth\")\n",
        "    print(\"        gt_path = r'{}'\".format(os.path.abspath(report['gt_file'])))\n",
        "    print(\"        # For .mat files: use scipy.io.loadmat(gt_path)\")\n",
        "    print(\"        # For .txt files: parse as text\")\n",
        "    print(\"        \")\n",
        "    print(\"        # Set image directory\")\n",
        "    print(\"        self.img_dir = r'{}'\".format(\n",
        "        os.path.abspath(report['train_dir']) if report['split'] == 'train' else os.path.abspath(report['test_dir'])\n",
        "    ))\n",
        "    print(\"        \")\n",
        "    print(\"        # Parse ground truth (example for text format)\")\n",
        "    print(\"        self.samples = []\")\n",
        "    print(\"        with open(gt_path, 'r') as f:\")\n",
        "    print(\"            for line in f:\")\n",
        "    print(\"                parts = line.strip().split()  # Adjust based on actual format\")\n",
        "    print(\"                if len(parts) >= 2:\")\n",
        "    print(\"                    img_name = parts[0]\")\n",
        "    print(\"                    text = ' '.join(parts[1:])\")\n",
        "    print(\"                    self.samples.append((img_name, text))\")\n",
        "    print(\"        \")\n",
        "    print(\"    def __len__(self):\")\n",
        "    print(\"        return len(self.samples)\")\n",
        "    print(\"    \")\n",
        "    print(\"    def __getitem__(self, idx):\")\n",
        "    print(\"        img_name, text = self.samples[idx]\")\n",
        "    print(\"        img_path = os.path.join(self.img_dir, img_name)\")\n",
        "    print(\"        image = Image.open(img_path).convert('L')  # Grayscale\")\n",
        "    print(\"        return image, text\")\n",
        "    print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 610
        },
        "id": "rkP-f5rgeYZd",
        "outputId": "0b2d6bd2-2961-48d0-bec8-5e11aaad5c28"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==================================================\n",
            "IIIT5K DATASET LOADER - SMART FIX\n",
            "==================================================\n",
            "==================================================\n",
            "ERROR: IIIT5K dataset not found!\n",
            "Please upload the dataset using:\n",
            "1. Google Drive: Mount Drive and specify path\n",
            "2. Direct upload: Click 'Upload' in Colab file browser\n",
            "3. Or download with: !wget http://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-5k-word-dataset/IIIT5K-Word_V3.0.tar\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "NEXT STEPS:\n",
            "1. Download dataset from: http://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-5k-word-dataset/\n",
            "2. Upload IIIT5K-Word_V3.0.tar to Colab\n",
            "   - Click folder icon > 'Upload' in left sidebar\n",
            "3. OR use direct download command:\n",
            "   !wget http://cvit.iiit.ac.in/research/projects/cvit-projects/the-iiit-5k-word-dataset/IIIT5K-Word_V3.0.tar\n",
            "==================================================\n",
            "\n",
            "==================================================\n",
            "DATASET LOADER READY - USE THIS IN YOUR OCR CODE\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'NoneType' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2578309801.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    159\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"DATASET LOADER READY - USE THIS IN YOUR OCR CODE\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"# Base directory: {os.path.abspath(report['base_dir'])}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"from torch.utils.data import Dataset\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"class IIIT5KDataset(Dataset):\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# deep_ocr_custom_split.py\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import AdamW\n",
        "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
        "from torch.nn.utils import clip_grad_norm_\n",
        "from tqdm import tqdm\n",
        "import string\n",
        "import scipy.io\n",
        "import random\n",
        "\n",
        "# ======================\n",
        "# 1. MODIFIED DATASET CLASS WITH CUSTOM SPLIT\n",
        "# ======================\n",
        "class CustomSplitIIIT5KDataset(Dataset):\n",
        "    def __init__(self, root_dir, split='train', img_height=32, img_width=128,\n",
        "                 train_samples=4500, test_samples=500, random_seed=42):\n",
        "        self.root_dir = root_dir\n",
        "        self.split = split\n",
        "        self.img_height = img_height\n",
        "        self.img_width = img_width\n",
        "        self.iiit5k_dir = os.path.join(root_dir, \"IIIT5K\")\n",
        "\n",
        "        # Set random seed for reproducible splits\n",
        "        random.seed(random_seed)\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "        # ‚úÖ STEP 1: Define vocabulary FIRST (critical fix!)\n",
        "        self.chars = string.ascii_letters + string.digits + \"!\\\"#$%&'()*+,-./:;<=>?@[\\\\]^_`{|}~\"\n",
        "        self.char_to_idx = {char: idx + 1 for idx, char in enumerate(self.chars)}\n",
        "        self.idx_to_char = {idx + 1: char for idx, char in enumerate(self.chars)}\n",
        "        self.idx_to_char[0] = '<BLANK>'\n",
        "        self.num_classes = len(self.chars) + 1  # 95 + 1\n",
        "\n",
        "        print(f\"Creating custom dataset split with {train_samples} train, {test_samples} test samples...\")\n",
        "\n",
        "        # Load all data from both original train and test sets\n",
        "        all_samples = []\n",
        "\n",
        "        # Load original train data\n",
        "        train_mat_file = os.path.join(self.iiit5k_dir, \"traindata.mat\")\n",
        "        train_data_dir = os.path.join(self.iiit5k_dir, \"train\")\n",
        "        if os.path.exists(train_mat_file):\n",
        "            all_samples.extend(self._load_mat_data(train_mat_file, train_data_dir, \"train\"))\n",
        "\n",
        "        # Load original test data\n",
        "        test_mat_file = os.path.join(self.iiit5k_dir, \"testdata.mat\")\n",
        "        test_data_dir = os.path.join(self.iiit5k_dir, \"test\")\n",
        "        if os.path.exists(test_mat_file):\n",
        "            all_samples.extend(self._load_mat_data(test_mat_file, test_data_dir, \"test\"))\n",
        "\n",
        "        print(f\"Total samples loaded: {len(all_samples)}\")\n",
        "\n",
        "        # Shuffle all samples for random split\n",
        "        random.shuffle(all_samples)\n",
        "\n",
        "        # Create custom train/test split\n",
        "        total_needed = train_samples + test_samples\n",
        "        if len(all_samples) < total_needed:\n",
        "            print(f\"‚ö†Ô∏è Warning: Only {len(all_samples)} samples available, but {total_needed} requested\")\n",
        "            train_samples = min(train_samples, len(all_samples) - test_samples)\n",
        "            test_samples = len(all_samples) - train_samples\n",
        "\n",
        "        if split == 'train':\n",
        "            self.samples = all_samples[:train_samples]\n",
        "            print(f\"‚úÖ Created training set with {len(self.samples)} samples\")\n",
        "        else:  # test\n",
        "            self.samples = all_samples[train_samples:train_samples + test_samples]\n",
        "            print(f\"‚úÖ Created test set with {len(self.samples)} samples\")\n",
        "\n",
        "        if self.samples:\n",
        "            sample_texts = [t for _, t in self.samples[:10]]\n",
        "            lengths = [len(t) for _, t in self.samples]\n",
        "            print(f\"Sample texts: {sample_texts}\")\n",
        "            print(f\"Text lengths: min={min(lengths)}, max={max(lengths)}, avg={np.mean(lengths):.1f}\")\n",
        "            print(f\"Vocabulary size: {self.num_classes}\")\n",
        "\n",
        "        # Image transform\n",
        "        self.transform = T.Compose([\n",
        "            T.ToPILImage(),\n",
        "            T.Resize((img_height, img_width), antialias=True),\n",
        "            T.Grayscale(),\n",
        "            T.ToTensor(),\n",
        "            T.Normalize((0.5,), (0.5,))  # [-1, 1]\n",
        "        ])\n",
        "\n",
        "    def _load_mat_data(self, mat_file, data_dir, original_split):\n",
        "        \"\"\"Load data from .mat file and return list of (img_path, text) tuples\"\"\"\n",
        "        samples = []\n",
        "\n",
        "        try:\n",
        "            mat_data = scipy.io.loadmat(mat_file)\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Failed to load {mat_file}: {e}\")\n",
        "            return samples\n",
        "\n",
        "        data_key = f\"{original_split}data\"\n",
        "        if data_key not in mat_data:\n",
        "            available = [k for k in mat_data.keys() if not k.startswith('__')]\n",
        "            print(f\"‚ùå Key '{data_key}' not found in {mat_file}. Available: {available}\")\n",
        "            return samples\n",
        "\n",
        "        data = mat_data[data_key]\n",
        "        N = data.shape[1] if data.shape[0] == 1 else data.shape[0]\n",
        "        print(f\"Processing {N} samples from {original_split} data...\")\n",
        "\n",
        "        for i in range(N):\n",
        "            try:\n",
        "                sample = data[0, i] if data.shape[0] == 1 else data[i, 0]\n",
        "\n",
        "                # Extract image path\n",
        "                img_name = self._safe_extract_string(sample[0])\n",
        "                if not img_name:\n",
        "                    img_filename = f\"sample_{i}.png\"\n",
        "                else:\n",
        "                    img_filename = os.path.basename(img_name.strip())\n",
        "                img_path = os.path.join(data_dir, img_filename)\n",
        "\n",
        "                # Extract text\n",
        "                text = self._safe_extract_string(sample[3])\n",
        "                text = self._clean_text(text)\n",
        "\n",
        "                if os.path.exists(img_path) and 1 <= len(text) <= 23:\n",
        "                    samples.append((img_path, text))\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        return samples\n",
        "\n",
        "    def _safe_extract_string(self, data):\n",
        "        \"\"\"Robustly extract string from .mat field (handles str, numeric array, object array)\"\"\"\n",
        "        try:\n",
        "            if isinstance(data, str):\n",
        "                return data.strip()\n",
        "\n",
        "            if isinstance(data, np.ndarray):\n",
        "                # Case 1: Numeric array (ASCII codes)\n",
        "                if np.issubdtype(data.dtype, np.number):\n",
        "                    chars = []\n",
        "                    for c in data.flatten():\n",
        "                        try:\n",
        "                            c_int = int(c)\n",
        "                            if 32 <= c_int <= 126:  # Printable ASCII\n",
        "                                chars.append(chr(c_int))\n",
        "                        except (ValueError, TypeError):\n",
        "                            continue\n",
        "                    return ''.join(chars).strip()\n",
        "\n",
        "                # Case 2: String array (U/S dtype)\n",
        "                elif data.dtype.kind in ['U', 'S']:\n",
        "                    return str(data.flatten()[0]).strip() if data.size > 0 else \"\"\n",
        "\n",
        "                # Case 3: Object array (common in scipy.io.loadmat)\n",
        "                elif data.dtype == np.object_:\n",
        "                    item = data.item() if data.size == 1 else data[0]\n",
        "                    if isinstance(item, str):\n",
        "                        return item.strip()\n",
        "                    return self._safe_extract_string(item)\n",
        "\n",
        "            return str(data).strip()\n",
        "        except Exception as e:\n",
        "            return \"\"\n",
        "\n",
        "    def _clean_text(self, text):\n",
        "        \"\"\"Keep only valid characters in full ASCII set\"\"\"\n",
        "        return ''.join(c for c in text if c in self.chars).strip()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, text = self.samples[idx]\n",
        "        img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "        if img is None:\n",
        "            print(f\"‚ö†Ô∏è Warning: Failed to load image {img_path}, using blank\")\n",
        "            img = np.ones((self.img_height, self.img_width), dtype=np.uint8) * 255\n",
        "\n",
        "        text_indices = [self.char_to_idx[char] for char in text if char in self.char_to_idx]\n",
        "        img_tensor = self.transform(img)\n",
        "        return img_tensor, text_indices, text\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 2. DEEP CRNN MODEL (UNCHANGED)\n",
        "# ======================\n",
        "class DeepCRNN(nn.Module):\n",
        "    def __init__(self, img_height=32, num_classes=96, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "        )\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=512,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=False,\n",
        "            dropout=0.3\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_size * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv = self.cnn(x)  # [B, 512, 1, W]\n",
        "        b, c, h, w = conv.size()\n",
        "        conv = conv.view(b, c * h, w).permute(2, 0, 1)  # [W, B, 512]\n",
        "        rnn_out, _ = self.rnn(conv)\n",
        "        rnn_out = self.dropout(rnn_out)\n",
        "        output = self.classifier(rnn_out)\n",
        "        return F.log_softmax(output, dim=2)\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 3. CTC UTILITIES (UNCHANGED)\n",
        "# ======================\n",
        "def ctc_collate_fn(batch):\n",
        "    images, text_indices_list, raw_texts = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "    targets = []\n",
        "    target_lengths = []\n",
        "    for text_indices in text_indices_list:\n",
        "        if len(text_indices) > 0:\n",
        "            targets.extend(text_indices)\n",
        "            target_lengths.append(len(text_indices))\n",
        "        else:\n",
        "            targets.append(0)\n",
        "            target_lengths.append(1)\n",
        "    return images, torch.tensor(targets), torch.tensor(target_lengths), raw_texts\n",
        "\n",
        "def ctc_decode(log_probs, idx_to_char, blank_idx=0):\n",
        "    \"\"\"Greedy CTC decoding\"\"\"\n",
        "    seq_len, batch_size, num_classes = log_probs.shape\n",
        "    predictions = []\n",
        "    for b in range(batch_size):\n",
        "        pred_indices = log_probs[:, b, :].argmax(dim=1).cpu().numpy()\n",
        "        decoded = []\n",
        "        prev_idx = blank_idx\n",
        "        for idx in pred_indices:\n",
        "            if idx != blank_idx and idx != prev_idx:\n",
        "                if idx in idx_to_char and idx_to_char[idx] != '<BLANK>':\n",
        "                    decoded.append(idx_to_char[idx])\n",
        "            prev_idx = idx\n",
        "        predictions.append(''.join(decoded))\n",
        "    return predictions\n",
        "\n",
        "def calculate_metrics(pred_texts, true_texts):\n",
        "    \"\"\"Calculate word and character accuracy\"\"\"\n",
        "    if not pred_texts or not true_texts:\n",
        "        return 0.0, 0.0\n",
        "    word_acc = sum(p == t for p, t in zip(pred_texts, true_texts)) / len(true_texts)\n",
        "    total_chars = sum(max(len(p), len(t)) for p, t in zip(pred_texts, true_texts))\n",
        "    correct_chars = sum(p[i] == t[i] for p, t in zip(pred_texts, true_texts) for i in range(min(len(p), len(t))))\n",
        "    char_acc = correct_chars / total_chars if total_chars > 0 else 0.0\n",
        "    return word_acc, char_acc\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 4. TRAINING LOOP WITH CUSTOM SPLIT\n",
        "# ======================\n",
        "def train_deep_ocr_custom_split():\n",
        "    print(\"üöÄ DEEP OCR TRAINING WITH CUSTOM SPLIT (4500 train, 500 test)\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # CONFIGURATION - Updated based on your dataset structure\n",
        "    dataset_root = \"/content/iiit5k_dataset\"  # Your extracted dataset path\n",
        "\n",
        "    # Verify dataset exists\n",
        "    print(f\"üîç Checking dataset root: {dataset_root}\")\n",
        "    print(f\"üìÅ Path exists: {os.path.exists(dataset_root)}\")\n",
        "\n",
        "    if not os.path.exists(dataset_root):\n",
        "        # Try alternative paths\n",
        "        alternative_paths = [\"./iiit5k_dataset\", \"/content/iiit5k_dataset\"]\n",
        "        for alt_path in alternative_paths:\n",
        "            if os.path.exists(alt_path):\n",
        "                dataset_root = alt_path\n",
        "                print(f\"‚úÖ Found dataset at alternative path: {dataset_root}\")\n",
        "                break\n",
        "        else:\n",
        "            print(f\"‚ùå Dataset root not found at any location\")\n",
        "            print(\"üí° Checked paths:\")\n",
        "            for path in [\"/content/iiit5k_dataset\"] + alternative_paths:\n",
        "                print(f\"   {path}: {'‚úÖ' if os.path.exists(path) else '‚ùå'}\")\n",
        "            return None\n",
        "    else:\n",
        "        print(f\"‚úÖ Dataset root found: {dataset_root}\")\n",
        "\n",
        "    # Verify IIIT5K subdirectory structure\n",
        "    iiit5k_path = os.path.join(dataset_root, \"IIIT5K\")\n",
        "    print(f\"üîç Checking IIIT5K path: {iiit5k_path}\")\n",
        "    print(f\"üìÅ IIIT5K exists: {os.path.exists(iiit5k_path)}\")\n",
        "\n",
        "    if os.path.exists(iiit5k_path):\n",
        "        contents = os.listdir(iiit5k_path)\n",
        "        print(f\"üìä IIIT5K contents: {contents}\")\n",
        "\n",
        "        # Check required files\n",
        "        required_items = ['train', 'test', 'traindata.mat', 'testdata.mat']\n",
        "        missing_items = [item for item in required_items if item not in contents]\n",
        "\n",
        "        if missing_items:\n",
        "            print(f\"‚ùå Missing required items: {missing_items}\")\n",
        "            return None\n",
        "        else:\n",
        "            print(\"‚úÖ All required dataset components found!\")\n",
        "    else:\n",
        "        print(f\"‚ùå IIIT5K subdirectory not found at: {iiit5k_path}\")\n",
        "        return None\n",
        "\n",
        "    batch_size = 32\n",
        "    num_epochs = 250\n",
        "    learning_rate = 1e-3\n",
        "    train_samples = 4500\n",
        "    test_samples = 500\n",
        "\n",
        "    print(f\"Custom split: {train_samples} train, {test_samples} test\")\n",
        "    print(f\"Batch size: {batch_size}\")\n",
        "    print(f\"Epochs: {num_epochs}\")\n",
        "    print(f\"Learning rate: {learning_rate}\")\n",
        "\n",
        "    # Load datasets with custom split\n",
        "    print(\"\\nCreating custom split datasets...\")\n",
        "    try:\n",
        "        train_dataset = CustomSplitIIIT5KDataset(\n",
        "            dataset_root, 'train', 32, 128, train_samples, test_samples\n",
        "        )\n",
        "        test_dataset = CustomSplitIIIT5KDataset(\n",
        "            dataset_root, 'test', 32, 128, train_samples, test_samples\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Dataset creation failed: {e}\")\n",
        "        return None\n",
        "\n",
        "    if len(train_dataset) == 0 or len(test_dataset) == 0:\n",
        "        print(\"‚ùå No valid data loaded.\")\n",
        "        return None\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
        "                              collate_fn=ctc_collate_fn, num_workers=0)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n",
        "                             collate_fn=ctc_collate_fn, num_workers=0)\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print(f\"\\nDevice: {device}\")\n",
        "\n",
        "    model = DeepCRNN(32, train_dataset.num_classes).to(device)\n",
        "    criterion = nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "\n",
        "    print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "    print(f\"Training batches: {len(train_loader)}\")\n",
        "    print(f\"Test batches: {len(test_loader)}\")\n",
        "\n",
        "    best_word_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_word_acc = 0.0\n",
        "        num_train_batches = 0\n",
        "\n",
        "        print(f\"\\nüìà Epoch {epoch+1}/{num_epochs}\")\n",
        "        pbar = tqdm(train_loader, desc=\"Training\")\n",
        "        for images, targets, target_lengths, raw_texts in pbar:\n",
        "            images = images.to(device)\n",
        "            targets = targets.to(device)\n",
        "            target_lengths = target_lengths.to(device)\n",
        "\n",
        "            log_probs = model(images)\n",
        "            input_lengths = torch.full((images.size(0),), log_probs.size(0), device=device, dtype=torch.long)\n",
        "            loss = criterion(log_probs, targets, input_lengths, target_lengths)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Log every 20 batches (adjusted for potentially more batches)\n",
        "            if num_train_batches % 20 == 0:\n",
        "                pred_texts = ctc_decode(log_probs.cpu(), train_dataset.idx_to_char)\n",
        "                word_acc, _ = calculate_metrics(pred_texts, raw_texts)\n",
        "                train_word_acc += word_acc\n",
        "                pbar.set_postfix({\"loss\": f\"{loss.item():.3f}\", \"word_acc\": f\"{word_acc:.3f}\"})\n",
        "\n",
        "            num_train_batches += 1\n",
        "\n",
        "        scheduler.step()\n",
        "        train_loss /= num_train_batches\n",
        "        train_word_acc /= (num_train_batches // 20 + 1)\n",
        "\n",
        "        # Validation\n",
        "        model.eval()\n",
        "        val_preds, val_truths = [], []\n",
        "        with torch.no_grad():\n",
        "            for images, targets, target_lengths, raw_texts in test_loader:\n",
        "                images = images.to(device)\n",
        "                log_probs = model(images)\n",
        "                pred_texts = ctc_decode(log_probs.cpu(), test_dataset.idx_to_char)\n",
        "                val_preds.extend(pred_texts)\n",
        "                val_truths.extend(raw_texts)\n",
        "\n",
        "        val_word_acc, val_char_acc = calculate_metrics(val_preds, val_truths)\n",
        "\n",
        "        print(f\"\\nüìä Epoch {epoch+1} Summary:\")\n",
        "        print(f\"  Training Loss: {train_loss:.4f}\")\n",
        "        print(f\"  Training Word Acc: {train_word_acc:.4f}\")\n",
        "        print(f\"  Validation Word Acc: {val_word_acc:.4f}\")\n",
        "        print(f\"  Validation Char Acc: {val_char_acc:.4f}\")\n",
        "        print(f\"  Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        if val_word_acc > best_word_acc:\n",
        "            best_word_acc = val_word_acc\n",
        "            torch.save({\n",
        "                'epoch': epoch + 1,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'val_word_acc': val_word_acc,\n",
        "                'idx_to_char': train_dataset.idx_to_char,\n",
        "                'num_classes': train_dataset.num_classes\n",
        "            }, 'deep_ocr_custom_split_model.pth')\n",
        "            print(f\"  ‚úÖ New best model saved! Word Acc: {best_word_acc:.4f}\")\n",
        "\n",
        "        # Sample predictions\n",
        "        print(f\"\\nüéØ Sample predictions:\")\n",
        "        indices = np.random.choice(len(val_preds), min(5, len(val_preds)), replace=False)\n",
        "        for i in indices:\n",
        "            print(f\"  True: '{val_truths[i]}' | Pred: '{val_preds[i]}'\")\n",
        "\n",
        "    print(f\"\\nüéâ Training completed!\")\n",
        "    print(f\"Best validation word accuracy: {best_word_acc:.4f}\")\n",
        "    return best_word_acc\n",
        "\n",
        "\n",
        "# ======================\n",
        "# 5. MAIN EXECUTION\n",
        "# ======================\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"‚ö° DEEP OCR SYSTEM WITH CUSTOM SPLIT\")\n",
        "    print(\"=\" * 50)\n",
        "    choice = input(\"\\nChoose option:\\n1. Train with custom split (4500 train, 500 test)\\n2. Exit\\nChoice: \")\n",
        "    if choice == \"1\":\n",
        "        accuracy = train_deep_ocr_custom_split()\n",
        "        if accuracy is not None:\n",
        "            print(f\"\\nüèÜ Final result: {accuracy:.4f} word accuracy\")\n",
        "        else:\n",
        "            print(\"\\n‚ùå Training failed. Check error messages above.\")\n",
        "    else:\n",
        "        print(\"Goodbye! üëã\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FLIEW-FkP2A",
        "outputId": "a9bb8393-41da-47e3-fc7d-4bc8883bbdf5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ö° DEEP OCR SYSTEM WITH CUSTOM SPLIT\n",
            "==================================================\n",
            "\n",
            "Choose option:\n",
            "1. Train with custom split (4500 train, 500 test)\n",
            "2. Exit\n",
            "Choice: 1\n",
            "üöÄ DEEP OCR TRAINING WITH CUSTOM SPLIT (4500 train, 500 test)\n",
            "============================================================\n",
            "üîç Checking dataset root: /content/iiit5k_dataset\n",
            "üìÅ Path exists: True\n",
            "‚úÖ Dataset root found: /content/iiit5k_dataset\n",
            "üîç Checking IIIT5K path: /content/iiit5k_dataset/IIIT5K\n",
            "üìÅ IIIT5K exists: False\n",
            "‚ùå IIIT5K subdirectory not found at: /content/iiit5k_dataset/IIIT5K\n",
            "\n",
            "‚ùå Training failed. Check error messages above.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test  the model\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "from torch.utils.data import Dataset\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "# ======================\n",
        "# 1. DEEP CRNN MODEL DEFINITION (MUST BE INCLUDED!)\n",
        "# ======================\n",
        "class DeepCRNN(nn.Module):\n",
        "    def __init__(self, img_height=32, num_classes=96, hidden_size=128):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(1, 64, 3, 1, 1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(64, 128, 3, 1, 1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2, 2),\n",
        "            nn.Conv2d(128, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(),\n",
        "            nn.Conv2d(256, 256, 3, 1, 1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Conv2d(256, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "            nn.Conv2d(512, 512, 3, 1, 1), nn.BatchNorm2d(512), nn.ReLU(), nn.MaxPool2d((2, 1), (2, 1)),\n",
        "        )\n",
        "        self.rnn = nn.LSTM(\n",
        "            input_size=512,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=2,\n",
        "            bidirectional=True,\n",
        "            batch_first=False,\n",
        "            dropout=0.3\n",
        "        )\n",
        "        self.classifier = nn.Linear(hidden_size * 2, num_classes)\n",
        "        self.dropout = nn.Dropout(0.4)\n",
        "        self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv = self.cnn(x)  # [B, 512, 1, W]\n",
        "        b, c, h, w = conv.size()\n",
        "        conv = conv.view(b, c * h, w).permute(2, 0, 1)  # [W, B, 512]\n",
        "        rnn_out, _ = self.rnn(conv)\n",
        "        rnn_out = self.dropout(rnn_out)\n",
        "        output = self.classifier(rnn_out)\n",
        "        return F.log_softmax(output, dim=2)\n",
        "\n",
        "# ======================\n",
        "# 2. CTC DECODE FUNCTION\n",
        "# ======================\n",
        "def ctc_decode(log_probs, idx_to_char, blank_idx=0):\n",
        "    \"\"\"Greedy CTC decoding\"\"\"\n",
        "    pred_indices = log_probs.argmax(dim=2).squeeze(1).cpu().numpy()  # [T]\n",
        "    decoded = []\n",
        "    prev_idx = blank_idx\n",
        "    for idx in pred_indices:\n",
        "        if idx != blank_idx and idx != prev_idx:\n",
        "            if idx in idx_to_char and idx_to_char[idx] != '<BLANK>':\n",
        "                decoded.append(idx_to_char[idx])\n",
        "        prev_idx = idx\n",
        "    return ''.join(decoded)\n",
        "\n",
        "# ======================\n",
        "# 3. INFERENCE SETUP\n",
        "# ======================\n",
        "model_path = \"/content/deep_ocr_custom_split_model (1).pth\"\n",
        "image_path = \"/content/WhatsApp Image 2025-09-04 at 10.02.39 AM.jpeg\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Image transform\n",
        "transform = T.Compose([\n",
        "    T.ToPILImage(),\n",
        "    T.Resize((32, 128), antialias=True),\n",
        "    T.Grayscale(),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "# ======================\n",
        "# 4. LOAD MODEL\n",
        "# ======================\n",
        "print(\"üöÄ Loading trained model...\")\n",
        "checkpoint = torch.load(model_path, map_location=device)\n",
        "num_classes = checkpoint['num_classes']\n",
        "idx_to_char = checkpoint['idx_to_char']\n",
        "\n",
        "# Now we can create the model\n",
        "model = DeepCRNN(img_height=32, num_classes=num_classes, hidden_size=128).to(device)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully with {num_classes} classes.\")\n",
        "\n",
        "# ======================\n",
        "# 5. LOAD AND PREPROCESS IMAGE\n",
        "# ======================\n",
        "def preprocess_image(img_path):\n",
        "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "    if img is None:\n",
        "        raise FileNotFoundError(f\"Cannot load image at {img_path}\")\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)  # To 3-channel for ToPILImage\n",
        "    return transform(img).unsqueeze(0)  # Add batch dim\n",
        "\n",
        "print(f\"üñºÔ∏è  Preprocessing image: {image_path}\")\n",
        "image_tensor = preprocess_image(image_path).to(device)\n",
        "\n",
        "# ======================\n",
        "# 6. RUN INFERENCE\n",
        "# ======================\n",
        "print(\"üîç Running inference...\")\n",
        "with torch.no_grad():\n",
        "    log_probs = model(image_tensor)  # [T, 1, num_classes]\n",
        "    predicted_text = ctc_decode(log_probs, idx_to_char)\n",
        "\n",
        "# ======================\n",
        "# 7. OUTPUT RESULT\n",
        "# ======================\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üéØ INFERENCE RESULT\")\n",
        "print(\"=\"*50)\n",
        "print(f\"Image: {image_path}\")\n",
        "print(f\"Predicted Text: '{predicted_text}'\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDjsLwqUeY5O",
        "outputId": "bb65de83-1ef5-4e42-d47b-cc600375bbde"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Loading trained model...\n",
            "‚úÖ Model loaded successfully with 95 classes.\n",
            "üñºÔ∏è  Preprocessing image: /content/WhatsApp Image 2025-09-04 at 10.02.39 AM.jpeg\n",
            "üîç Running inference...\n",
            "\n",
            "==================================================\n",
            "üéØ INFERENCE RESULT\n",
            "==================================================\n",
            "Image: /content/WhatsApp Image 2025-09-04 at 10.02.39 AM.jpeg\n",
            "Predicted Text: 'A'\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B0Vk_3abfaFR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}